{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import mcnulty_util as mcu\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating MAXIMUM data munging power\n",
      "Luther Preprocessing Successful Woo Woo!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = mcu.mcnulty_preprocessing()\n",
    "backup = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering by Issue Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "issue_year\n",
       "2007      251\n",
       "2008     1562\n",
       "2009     4716\n",
       "2010    11527\n",
       "2011    19729\n",
       "2012    49813\n",
       "2013    73194\n",
       "2014    74153\n",
       "2015    30832\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df\n",
    " .assign(issue_year=lambda x: x.issue_d.dt.year)\n",
    " .groupby('issue_year')\n",
    " .size()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.issue_d >= '2013') & (df.issue_d >= '2014')\n",
    "df = df.loc[mask, ]\n",
    "df.to_csv(os.path.join('Data', 'lendingclub_clean.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding_helper(input_df, column):\n",
    "    df = input_df.copy()\n",
    "    x = df.loc[:, column]\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(x)\n",
    "    df.loc[:, column] = le.transform(x)\n",
    "    \n",
    "    return df, le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loan Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$14,417'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_amount_mean = df.loan_amnt.mean()\n",
    "\"${:,.0f}\".format(loan_amount_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interest Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'14.26%'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interest_rate_mean = df.int_rate.mean()\n",
    "\"{:,.2f}%\".format(interest_rate_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annual Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$74,737'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annual_income_mean = df.annual_inc.mean()\n",
    "\"${:,.0f}\".format(annual_income_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debt-to-Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$17.84'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dti_mean = df.dti.mean()\n",
    "\"${:,.2f}\".format(dti_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employment Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10+ years'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.emp_length.mode().values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, le_emp_length = label_encoding_helper(df, 'emp_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'36 months'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.term.mode().values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, le_term = label_encoding_helper(df, 'term')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'debt_consolidation'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.purpose.mode().values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, le_purpose = label_encoding_helper(df, 'purpose')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.grade.mode().values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, le_grade = label_encoding_helper(df, 'grade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'emp_length', 'term', 'purpose', 'grade']\n",
    "dependent = 'default'\n",
    "model_name = 'Random Forest'\n",
    "X, y = df.loc[:, features], df.loc[:, dependent]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "rf = RandomForestClassifier()\n",
    "%time rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.05 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'emp_length', 'term', 'purpose', 'grade']\n",
    "dependent = 'default'\n",
    "model_name = 'Random Forest'\n",
    "X, y = df.loc[:, features], df.loc[:, dependent]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "%time rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy:   99.9972%\n",
      "Test Set Accuracy:       73.1138%\n"
     ]
    }
   ],
   "source": [
    "yhat_train = rf.predict(X_train)\n",
    "training_accuracy = metrics.accuracy_score(y_train, yhat_train)\n",
    "yhat_test = rf.predict(X_test)\n",
    "test_accuracy = metrics.accuracy_score(y_test, yhat_test)\n",
    "accuracy_str = \"Training Set Accuracy: {:>10.4%}\\nTest Set Accuracy: {:>14.4%}\".format(training_accuracy, test_accuracy)\n",
    "print(accuracy_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve\n",
    "\n",
    "[Reference Link](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities and keep probabilities for the positive outcome only\n",
    "y_pred_probs = rf.predict_proba(X_test)[:, 1]\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "# calculate scores\n",
    "ns_auc = metrics.roc_auc_score(y_test, ns_probs)\n",
    "rf_auc = metrics.roc_auc_score(y_test, y_pred_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Random Forest: ROC AUC=%.3f' % (rf_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = metrics.roc_curve(y_test, ns_probs)\n",
    "rf_fpr, rf_tpr, _ = metrics.roc_curve(y_test, y_pred_probs)\n",
    "# plot the roc curve for the model\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "plt.plot(rf_fpr, rf_tpr, marker='.', label='Random Forest')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve\n",
    "\n",
    "[Reference Link](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities and keep probabilities for the positive outcome only\n",
    "y_pred_probs = rf.predict_proba(X_test)[:, 1]\n",
    "# predict class values\n",
    "y_pred = rf.predict(X_test)\n",
    "# calculate precision and recall for each threshold\n",
    "rf_precision, rf_recall, thresholds = metrics.precision_recall_curve(y_test, y_pred_probs)\n",
    "# calculate scores\n",
    "#rf_f1, rf_auc = metrics.f1_score(y_test, y_pred), metrics.auc(rf_precision, rf_recall)\n",
    "# summarize scores\n",
    "#print('Random Forest: f1=%.3f auc=%.3f' % (rf_f1, rf_auc))\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(y_pred[y_pred==1]) / len(y_pred)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(rf_recall, rf_precision, marker='.', label='Random Forest')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_recall_curve(y_test, y_pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall at different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_threshold(model, X, threshold):\n",
    "    y_pred_prob = rf.predict_proba(X_test)[:, 1]\n",
    "    mask = y_pred_prob > threshold\n",
    "    y_pred = np.where(mask, 1, 0)\n",
    "    return y_pred\n",
    "\n",
    "\"\"\" Check the function is written correctly \"\"\"\n",
    "sum(predict_with_threshold(rf, X_test, 0.5) != rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_and_recall_with_threshold(model, X, y_true, threshold):\n",
    "    y_pred = predict_with_threshold(model, X, threshold)\n",
    "    recall_score = metrics.recall_score(y_true, y_pred)\n",
    "    precision_score = metrics.precision_score(y_true, y_pred)\n",
    "    return precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_and_recall_scores = dict()\n",
    "thresholds = [0.1 * x for x in range(0, int(1/0.1) + 1)]\n",
    "for threshold in thresholds:\n",
    "    precision_score, recall_score = precision_and_recall_with_threshold(rf, X_test, y_test, threshold)\n",
    "    precision_and_recall_scores[threshold] = [precision_score, recall_score]\n",
    "df_pr = pd.DataFrame.from_dict(precision_and_recall_scores, orient='index', columns=['Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pr.applymap('{:0.2%}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities and keep probabilities for the positive outcome only\n",
    "y_pred_probs = rf.predict_proba(X_test)[:, 1]\n",
    "# predict class values\n",
    "y_pred = rf.predict(X_test)\n",
    "# calculate precision and recall for each threshold\n",
    "rf_precision, rf_recall = df_pr.Precision, df_pr.Recall\n",
    "# calculate scores\n",
    "#rf_f1, rf_auc = metrics.f1_score(y_test, y_pred), metrics.auc(rf_precision, rf_recall)\n",
    "# summarize scores\n",
    "#print('Random Forest: f1=%.3f auc=%.3f' % (rf_f1, rf_auc))\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(y_pred[y_pred==1]) / len(y_pred)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(rf_recall, rf_precision, marker='.', label='Random Forest')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# add labels\n",
    "for x,y, threshold in zip(rf_recall, rf_precision, df_pr.index):\n",
    "    label = \"{:.2f}\".format(threshold)\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_pr.loc[[0.5, 0.1], ]\n",
    " .applymap('{:0.2%}'.format)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blog Objects and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder_map(le):\n",
    "    labels = list(le.classes_)\n",
    "    labels_encoded = le.transform(labels)\n",
    "    le_encoder_map = dict(zip(labels, labels_encoded))\n",
    "    return le_encoder_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_length_map = label_encoder_map(le_emp_length)\n",
    "emp_length_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_map = label_encoder_map(le_term)\n",
    "term_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose_map = label_encoder_map(le_purpose)\n",
    "purpose_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_map = label_encoder_map(le_grade)\n",
    "grade_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = pd.DataFrame({\n",
    "        'loan_amnt': 13658.0, \n",
    "        'int_rate': 17, \n",
    "        'annual_inc': 150000, \n",
    "        'dti': 16.7, \n",
    "        'emp_length': emp_length_map['10+ years'], \n",
    "        'term': term_map['36 months'], \n",
    "        'purpose': purpose_map['debt_consolidation'], \n",
    "        'grade': grade_map['B'],\n",
    "    },\n",
    "    index=[0]\n",
    ")\n",
    "rf.predict_proba(row)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'rf.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(rf, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
