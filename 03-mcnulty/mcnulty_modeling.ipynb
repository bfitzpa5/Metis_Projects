{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lending Club Loan Data Modeling\n",
    "\n",
    "In this section we will attempt to determine the best model to predict whether or not a borrower will default in the Lending Club Loan data.\n",
    "\n",
    "Before beginning, we'll define our **_Satisficing_** and **_Optimizing_** metrics. Andrew Ng recommends outlining these before beginning in the _deeplearning.ai_ course named _Structuring Machine Learning Projects_.\n",
    "\n",
    "After, we'll get down and dirty with some data cleaning to get this dataset in tip-top shape and ready to be modeled.\n",
    "\n",
    "We then start the modeling, beginning with a **_Logistic Regresion_** model, using **_Forward Selection_** to determine the features. We will then try a **_K-Nearest Neighbors Classifier_** and end with a **_Random Forest_** and some hyperparameter tuning. \n",
    "\n",
    "After we'll wrap it all up with a summary of what we have learned.\n",
    "\n",
    "First though, let's do our usual import of a billions packages so we're ready to machine learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import mcnulty_util as mcu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Project Goals](#project_goals)\n",
    "2. [Data Cleaning](#data_cleaning)\n",
    "3. [Logistic Regressions](#log_reg)\n",
    "    1. [Single Feature](#log_reg_one)\n",
    "    2. [Multiple Features](#log_reg_mult)\n",
    "    3. [Visualization of Best Model](#log_reg_viz)\n",
    "    4. [Hyperparameter Tuning with Grid Search](#log_reg_hyperparams)\n",
    "4. [K-Nearest Neighbors](#knn)\n",
    "5. [Random Forest](#rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"project_goals\"></a>\n",
    "# Project Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goals is to provide investors with a model that allows them to invest in loans with a high confidence that the borrower will not default. This will minimize losses to that investor by making sure their prinicpal is secure.\n",
    "\n",
    "As we our cautious investors, we want our model to predict default if it is not confident either way. To do this we will optimize **_Recall_**, also known as the **_True Positive Rate_**, which is defined as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "Recall = True\\ Positive\\ Rate = \\frac{TP} {TP + FN}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "TP = Number\\ of\\ True\\ Positives\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "FP = Number\\ of\\ False\\ Negatives\n",
    "\\end{equation*}\n",
    "\n",
    "**_Recall_** is a measure of a classifiers **_completeness_**. \n",
    "In this instance, recall measures how many of the defaulters we predicted defaulted.\n",
    "\n",
    "However, if we optimize too much towards recall we will end up predicting default the whole time. \n",
    "This will give us 100% recall, but not a good model. \n",
    "To avoid this we'll also include **_precision_**. \n",
    "**_Precision_**, also known as the **_Positive Predictive Value._**, is defined as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "Precision = Positive\\ Predictive\\ Value = \\frac{TP} {TP + FP}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "TP = Number\\ of\\ True\\ Positives\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "FP = Number\\ of\\ False\\ Positives\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Since we want to balance precision and recall, we'll use the **_F1 Score_**, which is the harmonic mean of the two.\n",
    "\n",
    "This means **_F1 Score_** will be our optimizing metric. We'll want to find a model with above 80% precision as a satisficing metric.\n",
    "\n",
    "Before we get to that, let's get down and dirty with some data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data_cleaning\"></a>\n",
    "# Data Cleaning\n",
    "\n",
    "I have moved our data cleaning to the _mcnulty_util.py_ model to keep modularity. \n",
    "\n",
    "In this function, we filter out loan status' that don't apply per the EDA file. We also subsetted the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mcu.mcnulty_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"log_reg\"></a>\n",
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"log_reg_one\"></a>\n",
    "## Single Feature Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independents = [\n",
    "    ['dti'],\n",
    "    ['int_rate'],\n",
    "    ['annual_inc'],\n",
    "    ['loan_amnt'],\n",
    "    ['revol_bal'],\n",
    "    ['term'],\n",
    "    ['delinq_2yrs'],\n",
    "    ['home_ownership'],\n",
    "    ['grade'],\n",
    "    ['purpose'],\n",
    "    ['emp_length']]\n",
    "dependent = 'default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list()\n",
    "for variable in independents:\n",
    "    X, y = df.loc[:, variable], df.loc[:, dependent]\n",
    "    clf = LogisticRegression(C=1000000, penalty='l1')\n",
    "    if X.iloc[:, 0].dtype not in [np.float64, np.int64]:\n",
    "        enc = OneHotEncoder()\n",
    "        X = enc.fit_transform(X)\n",
    "        record = mcu.log_clf_model(clf, 'Logistic Regression', X, y, variable)\n",
    "        results.append(record)\n",
    "    else:\n",
    "        for degree in range(1, 4):\n",
    "            if degree == 1:\n",
    "                LogisticRegression(C=1000000, penalty='l1')\n",
    "                record = mcu.log_clf_model(clf, 'Logistic Regression', X, y, variable)\n",
    "                results.append(record)\n",
    "            else:\n",
    "                clf = Pipeline([('poly', PolynomialFeatures(degree)), \n",
    "                                ('clf', LogisticRegression(C=1000000, penalty='l1'))])\n",
    "                record = mcu.log_clf_model(clf, 'Logistic Regression', X, y, variable, degree)\n",
    "                results.append(record)\n",
    "# Let's also add a bias model\n",
    "X = np.ones((df.shape[0], 1))\n",
    "y = df.loc[:, dependent]\n",
    "clf = LogisticRegression(C=1000000, penalty='l1')\n",
    "results.append(mcu.log_clf_model(clf, 'Logistic Regression', X, y, 'bias'))\n",
    "(mcu.results_to_df(results)\n",
    " .pipe(mcu.scores_formatted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all our models except 2 are guess 100 percent non-default. This is common with imbalanced classes. We are basically dealing with a high-bias problem here. We need to add features to **_reduce bias_** and **_add variance_**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"log_reg_mult\"></a>\n",
    "## Multiple Features\n",
    "\n",
    "To add some variance, we'll now add models with two or three features, with each numeric dependent variable having polynomial tranformations from 1-3 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features_tuple in itertools.combinations(list(mcu.independents.keys()), 2):\n",
    "    features = list(features_tuple)\n",
    "    if mcu.independents[features[0]] == 'dummy' and mcu.independents[features[1]] == 'dummy':\n",
    "        clf = LogisticRegression(C=1000000, penalty='l1')\n",
    "        pipeline = mcu.clf_pipeline(clf, features, degree)\n",
    "        record = mcu.log_clf_model(pipeline, 'Logistic Regression', df, y, features, 1)\n",
    "        results.append(record)\n",
    "    else: \n",
    "        for degree in range(1, 4):\n",
    "            clf = LogisticRegression(C=1000000, penalty='l1')\n",
    "            pipeline = mcu.clf_pipeline(clf, features, degree)\n",
    "            record = mcu.log_clf_model(pipeline, 'Logistic Regression', df, y, features, degree)\n",
    "            results.append(record)\n",
    "for features_tuple in itertools.combinations(list(mcu.independents.keys()), 3):\n",
    "    features = list(features_tuple)\n",
    "    if (    mcu.independents[features[0]] == 'dummy'\n",
    "        and mcu.independents[features[1]] == 'dummy'\n",
    "        and mcu.independents[features[2]] == 'dummy'):\n",
    "        clf = LogisticRegression(C=1000000, penalty='l1')\n",
    "        pipeline = mcu.clf_pipeline(clf, features, degree)\n",
    "        record = mcu.log_clf_model(pipeline, 'Logistic Regression', df, y, features, 1)\n",
    "        results.append(record)\n",
    "    else: \n",
    "        for degree in range(1, 4):\n",
    "            clf = LogisticRegression(C=1000000, penalty='l1')\n",
    "            pipeline = mcu.clf_pipeline(clf, features, degree)\n",
    "            record = mcu.log_clf_model(pipeline, 'Logistic Regression', df, y, features, degree)\n",
    "            results.append(record)\n",
    "(mcu.results_to_df(results)\n",
    " .pipe(scores_formatted)\n",
    " .head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(unpack_list(results_to_df(results)\n",
    "         .pipe(scores_formatted)\n",
    "         .head(15)\n",
    "         .features.tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"log_reg_viz\"></a>\n",
    "## Visualization of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['dti', 'int_rate']\n",
    "dependent = 'default'\n",
    "X, y = df.loc[:, features], df.loc[:, dependent]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11,\n",
    "                                                    stratify=y)\n",
    "degree = 2\n",
    "clf = mcu.clf_pipeline(LogisticRegression(), features, degree)\n",
    "clf.fit(X_train, y_train)\n",
    "ax = mcu.plot_estimator(clf, X_test,y_test)\n",
    "ax.set(title='Defaulters by DTI and Interest Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not great, but our classifer is definitely telling us people with higher Debt-to-Income Ratios and higher Interest Rates are more likely to default, which makes sense. We can tell from this that the data doesn't provide us with a clean split unfortunately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"#log_reg_hyperparams\"></a>\n",
    "## Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['dti', 'int_rate', 'emp_length', 'home_ownership', 'purpose',\n",
    "            'delinq_2yrs','revol_bal', 'loan_amnt', 'grade', 'term']\n",
    "degree = 2\n",
    "X, y = df.loc[:, features], df.loc[:, dependent]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11,\n",
    "                                                    stratify=y)\n",
    "pipeline = clf_pipeline(LogisticRegression(), features, degree)\n",
    "weight_space = np.linspace(0.05, 0.95, 20)\n",
    "class_weights = [{0: x, 1: 1.0-x} for x in weight_space]\n",
    "hyperparameters = dict(clf__class_weight=class_weights)\n",
    "gs = GridSearchCV(pipeline, hyperparameters, scoring='f1', cv=5)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Class Weights : {}\".format(pd.DataFramegs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_desc = 'Logistic Regression with Class Weights'\n",
    "#class_weight = gs.best_params_['clf__class_weight']\n",
    "features = ['dti', 'int_rate', 'emp_length', 'home_ownership', 'purpose',\n",
    "            'delinq_2yrs','revol_bal', 'loan_amnt', 'grade', 'term']\n",
    "class_weight = {0: 0.23947368421052628, 1: 0.7605263157894737}\n",
    "logr = LogisticRegression(class_weight=class_weight)\n",
    "X, y = df.loc[:, features], df.loc[:, dependent]\n",
    "pipeline = clf_pipeline(logr, features, degree)\n",
    "results.append(mcu.log_clf_model(pipeline, model_desc, X, y, features, degree=degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_desc = 'Logistic Regression with Class Weights'\n",
    "degree = 2\n",
    "features = ['dti', 'int_rate', 'emp_length', 'home_ownership', 'purpose',\n",
    "            'delinq_2yrs','revol_bal', 'loan_amnt', 'grade', 'term', 'installment']\n",
    "class_weight = {0: 0.23947368421052628, 1: 0.7605263157894737}\n",
    "logr = LogisticRegression(class_weight=class_weight)\n",
    "X, y = df.loc[:, features], df.loc[:, dependent]\n",
    "pipeline = clf_pipeline(logr, features, degree)\n",
    "results.append(mcu.log_clf_model(pipeline, model_desc, X, y, features, degree=degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_desc = 'Logistic Regression with Class Weights'\n",
    "degree = 3\n",
    "features = ['dti', 'int_rate', 'emp_length', 'home_ownership', 'purpose',\n",
    "            'delinq_2yrs','revol_bal', 'loan_amnt', 'grade', 'term', 'installment']\n",
    "class_weight = {0: 0.23947368421052628, 1: 0.7605263157894737}\n",
    "logr = LogisticRegression(class_weight=class_weight)\n",
    "X, y = df.loc[:, features], df.loc[:, dependent]\n",
    "pipeline = clf_pipeline(logr, features, degree)\n",
    "results.append(mcu.log_clf_model(pipeline, model_desc, X, y, features, degree=degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_desc = 'Logistic Regression with Class Weights'\n",
    "degree = 3\n",
    "features = ['dti', 'int_rate', 'emp_length', 'home_ownership', 'purpose',\n",
    "            'delinq_2yrs','revol_bal', 'loan_amnt', 'grade', 'term', 'installment',\n",
    "            'addr_state']\n",
    "class_weight = {0: 0.23947368421052628, 1: 0.7605263157894737}\n",
    "logr = LogisticRegression(class_weight=class_weight)\n",
    "X, y = df.loc[:, features], df.loc[:, dependent]\n",
    "pipeline = clf_pipeline(logr, features, degree)\n",
    "results.append(mcu.log_clf_model(pipeline, model_desc, X, y, features, degree=degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(results_to_df(results)\n",
    " .pipe(scores_formatted)\n",
    " .head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"knn\"></a>\n",
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's try the K-Nearest Neighbors algorithm on the data. We'll pick features by those with the highest correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.corr()\n",
    " .loc[:, ['default']]\n",
    " .drop('default', axis=0)\n",
    " .rename(columns={'default': 'correlation'})\n",
    " .assign(correlation_abs=lambda x: x.correlation.abs())\n",
    " .sort_values('correlation_abs', ascending=False)\n",
    " .head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, dependent = ['out_prncp', 'int_rate'], ['default']\n",
    "X, y = df.loc[:, features], df.loc[:, dependent]\n",
    "scaler = StandardScaler()\n",
    "X_trans = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_trans, y, test_size=0.2, random_state=11,\n",
    "                                                    stratify=y)\n",
    "n_neighbors = [2, 4, 8, 16, 32]\n",
    "train_results = list()\n",
    "test_results = list()\n",
    "for neighbors in n_neighbors:\n",
    "    knc = KNeighborsClassifier(n_neighbors=neighbors, n_jobs=-1)\n",
    "    knc.fit(X_train, y_train)\n",
    "    y_train_hat = knc.predict(X_train)\n",
    "    f1_train_score = metrics.f1_score(y_train, y_train_hat)\n",
    "    train_results.append(f1_train_score)\n",
    "    y_test_hat = knc.predict(X_test)\n",
    "    f1_test_score = metrics.f1_score(y_test, y_test_hat)\n",
    "    test_results.append(f1_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(n_neighbors, train_results, c='blue', label='Training Set')\n",
    "ax.plot(n_neighbors, test_results, c='red', label='Training Set')\n",
    "ax.set(title='K-Nearest Neighbors\\nDefault by Outstanding Principal and Interest Rate',\n",
    "       xlabel=\"Number of Neighbors\", ylabel=\"F1 Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best KNN is one with 2 neighbors. Let's check out a visual of this for funzies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rf\"></a>\n",
    "# Random Forest\n",
    "\n",
    "We did some great work with our logistic regression modeling, but let's see if we can obtain a little more accuracy with a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['dti', 'int_rate', 'emp_length', 'home_ownership', 'purpose',\n",
    "            'delinq_2yrs','revol_bal', 'loan_amnt', 'grade', 'term', 'installment', 'addr_state']\n",
    "dependent = 'default'\n",
    "model_name = 'Random Forest'\n",
    "X, y = df.loc[:, features], df.loc[:, dependent]\n",
    "rf = RandomForestClassifier()\n",
    "pipeline = mcu.clf_pipeline(rf, features, 1)\n",
    "mcu.log_clf_model(pipeline, model_name, X, y, features, degree=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Hyperparameter Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parameters \"\"\"\n",
    "features = ['dti', 'int_rate', 'emp_length', 'home_ownership', 'purpose',\n",
    "            'delinq_2yrs','revol_bal', 'loan_amnt', 'grade', 'term', 'installment', 'addr_state']\n",
    "degree = 1\n",
    "X, y = df.loc[:, features], df.loc[:, dependent]\n",
    "\n",
    "\"\"\" Preprocessing \"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11,\n",
    "                                                    stratify=y)\n",
    "transformer_list = mcu.feature_transformer_list(features, degree)\n",
    "feats = FeatureUnion(transformer_list=transformer_list)\n",
    "X_train_trans = feats.fit_transform(X_train)\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_trans, y_train)\n",
    "\n",
    "\"\"\" Grid Search Hyperparameters \"\"\"\n",
    "# Number of trees in random forest\n",
    "n_estimators = [x for x in range(200, 2000, 100)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [None] + [x for x in [x for x in range(10, 110, 10)]]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2**r for r in range(1, 4)]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [2**r for r in range(0, 3)]\n",
    "\n",
    "\"\"\" Grid Search \"\"\"\n",
    "hyperparameters = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "gs = GridSearchCV(rf, hyperparameters, scoring='f1', cv=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
