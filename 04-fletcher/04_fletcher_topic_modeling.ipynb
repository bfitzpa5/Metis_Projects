{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(\n",
    "    'Data',\n",
    "    'kickstarter_data.json'\n",
    ")\n",
    "df = pd.read_json(fname, 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.4% of pitches (2,274 out of 2,384 total) have stories\n"
     ]
    }
   ],
   "source": [
    "total_pitches = df.shape[0]\n",
    "pitches_with_stories = df.loc[~df.story.isna(), ].shape[0]\n",
    "percent_with_story = pitches_with_stories / total_pitches\n",
    "args = (percent_with_story, pitches_with_stories, total_pitches)\n",
    "print('{:.1%} of pitches ({:0,.0f} out of {:0,.0f} total) have stories'.format(*args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We''ll fill the remaining pitches with their project description\n",
    "\"\"\"\n",
    "df.story = df.story.fillna(df.project_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(631174, 2384)\n"
     ]
    }
   ],
   "source": [
    "kickstarter_corpus = df.story.values\n",
    "kwargs = dict(\n",
    "    ngram_range=(1, 2),  \n",
    "    stop_words='english',\n",
    "    token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    ")\n",
    "cv = CountVectorizer(**kwargs)\n",
    "cv.fit(kickstarter_corpus)\n",
    "counts = cv.transform(kickstarter_corpus).transpose()\n",
    "print(counts.shape)\n",
    "corpus = matutils.Sparse2Corpus(counts)\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.002*\"brown\" + 0.002*\"wallet\" + 0.002*\"cards\" + 0.002*\"black\" + 0.002*\"available\" + 0.001*\"cash\" + 0.001*\"blue\" + 0.001*\"credit\" + 0.001*\"black brown\" + 0.001*\"available black\" + 0.001*\"red\" + 0.001*\"leather\" + 0.001*\"card\" + 0.001*\"redskins\" + 0.001*\"cowboys\" + 0.001*\"brown blue\" + 0.001*\"slots\" + 0.001*\"grey\" + 0.001*\"credit cards\" + 0.001*\"available brown\"'),\n",
       " (1,\n",
       "  '0.001*\"stove\" + 0.000*\"youth culture\" + 0.000*\"lash\" + 0.000*\"sternum strap\" + 0.000*\"sternum\" + 0.000*\"fuel\" + 0.000*\"core\" + 0.000*\"puk\" + 0.000*\"core puk\" + 0.000*\"cis\" + 0.000*\"lash binder\" + 0.000*\"museum youth\" + 0.000*\"binder\" + 0.000*\"pot\" + 0.000*\"shart\" + 0.000*\"taped\" + 0.000*\"aurora\" + 0.000*\"simmer\" + 0.000*\"core aurora\" + 0.000*\"youth\"'),\n",
       " (2,\n",
       "  '0.003*\"use\" + 0.002*\"design\" + 0.002*\"product\" + 0.002*\"skin\" + 0.002*\"water\" + 0.002*\"products\" + 0.002*\"watch\" + 0.002*\"oil\" + 0.002*\"bag\" + 0.002*\"table\" + 0.002*\"strap\" + 0.001*\"easy\" + 0.001*\"used\" + 0.001*\"technology\" + 0.001*\"time\" + 0.001*\"using\" + 0.001*\"fabric\" + 0.001*\"quality\" + 0.001*\"pocket\" + 0.001*\"tool\"'),\n",
       " (3,\n",
       "  '0.009*\"pins\" + 0.008*\"pin\" + 0.005*\"kickstarter\" + 0.004*\"enamel\" + 0.004*\"pledge\" + 0.004*\"campaign\" + 0.004*\"unlocked\" + 0.003*\"album\" + 0.002*\"shipping\" + 0.002*\"designs\" + 0.002*\"goal\" + 0.002*\"hard\" + 0.002*\"hard enamel\" + 0.002*\"design\" + 0.002*\"stretch\" + 0.002*\"add\" + 0.002*\"band\" + 0.002*\"goals\" + 0.002*\"backing\" + 0.002*\"production\"'),\n",
       " (4,\n",
       "  '0.005*\"und\" + 0.005*\"die\" + 0.003*\"der\" + 0.002*\"ich\" + 0.002*\"zu\" + 0.002*\"ist\" + 0.002*\"das\" + 0.001*\"auf\" + 0.001*\"von\" + 0.001*\"mit\" + 0.001*\"es\" + 0.001*\"den\" + 0.001*\"ein\" + 0.001*\"wir\" + 0.001*\"eine\" + 0.001*\"im\" + 0.001*\"um\" + 0.001*\"haben\" + 0.001*\"sich\" + 0.001*\"wird\"'),\n",
       " (5,\n",
       "  '0.004*\"dice\" + 0.004*\"tea\" + 0.003*\"deck\" + 0.001*\"tray\" + 0.001*\"set\" + 0.001*\"decks\" + 0.001*\"gong\" + 0.001*\"gong fu\" + 0.001*\"fu\" + 0.001*\"coffee\" + 0.001*\"gems\" + 0.001*\"add\" + 0.001*\"oolong\" + 0.001*\"roses\" + 0.001*\"teas\" + 0.001*\"dice set\" + 0.001*\"roast\" + 0.001*\"oracle\" + 0.001*\"tarot\" + 0.001*\"cups\"'),\n",
       " (6,\n",
       "  '0.010*\"la\" + 0.008*\"que\" + 0.007*\"el\" + 0.006*\"en\" + 0.004*\"una\" + 0.003*\"para\" + 0.003*\"los\" + 0.003*\"es\" + 0.003*\"por\" + 0.002*\"las\" + 0.002*\"se\" + 0.002*\"del\" + 0.002*\"mi\" + 0.002*\"su\" + 0.002*\"al\" + 0.001*\"lo\" + 0.001*\"este\" + 0.001*\"como\" + 0.001*\"en el\" + 0.001*\"proyecto\"'),\n",
       " (7,\n",
       "  '0.004*\"game\" + 0.004*\"new\" + 0.004*\"time\" + 0.003*\"book\" + 0.003*\"make\" + 0.003*\"like\" + 0.003*\"world\" + 0.003*\"project\" + 0.003*\"help\" + 0.003*\"want\" + 0.003*\"kickstarter\" + 0.003*\"just\" + 0.003*\"work\" + 0.003*\"people\" + 0.002*\"film\" + 0.002*\"story\" + 0.002*\"play\" + 0.002*\"life\" + 0.002*\"art\" + 0.002*\"ve\"'),\n",
       " (8,\n",
       "  '0.004*\"la\" + 0.004*\"et\" + 0.004*\"en\" + 0.003*\"je\" + 0.003*\"le\" + 0.002*\"les\" + 0.002*\"est\" + 0.002*\"pour\" + 0.002*\"une\" + 0.002*\"des\" + 0.002*\"dans\" + 0.002*\"ai\" + 0.001*\"du\" + 0.001*\"que\" + 0.001*\"di\" + 0.001*\"qui\" + 0.001*\"guadalajara\" + 0.001*\"vous\" + 0.001*\"avec\" + 0.001*\"photo\"'),\n",
       " (9,\n",
       "  '0.003*\"iron\" + 0.002*\"pans\" + 0.001*\"cookware\" + 0.001*\"wrought\" + 0.001*\"usa\" + 0.001*\"cast\" + 0.001*\"solidteknics\" + 0.001*\"cast iron\" + 0.001*\"bees\" + 0.001*\"medical\" + 0.001*\"new\" + 0.001*\"card\" + 0.001*\"festival\" + 0.001*\"edition\" + 0.001*\"wrought iron\" + 0.001*\"line\" + 0.001*\"film festival\" + 0.001*\"plastic\" + 0.001*\"com\" + 0.001*\"steel\"')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus=corpus, num_topics=10, id2word=id2word, passes=10)\n",
    "lda.print_topics(num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTopic 5 is French and Spanish let's get those out of here!\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Topic 5 is French and Spanish let\\'s get those out of here!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 125 projects in different languages\n"
     ]
    }
   ],
   "source": [
    "other_language_mask = df.story.str.contains(' le | la | je | das ') # picked a subset of words from other languages to filter with\n",
    "print(\"There are {} projects in different languages\".format(df.loc[other_language_mask, ].shape[0]))\n",
    "df = df.loc[~other_language_mask, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(574379, 2259)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"game\" + 0.003*\"new\" + 0.003*\"time\" + 0.003*\"make\" + 0.003*\"like\" + 0.003*\"book\" + 0.003*\"kickstarter\" + 0.003*\"project\" + 0.003*\"help\" + 0.003*\"want\" + 0.003*\"world\" + 0.002*\"just\" + 0.002*\"film\" + 0.002*\"work\" + 0.002*\"design\" + 0.002*\"people\" + 0.002*\"life\" + 0.002*\"play\" + 0.002*\"cards\" + 0.002*\"campaign\"'),\n",
       " (1,\n",
       "  '0.001*\"bamboo\" + 0.001*\"fact bamboo\" + 0.000*\"fun fact\" + 0.000*\"gaxmoor\" + 0.000*\"undies\" + 0.000*\"tea\" + 0.000*\"delorean\" + 0.000*\"lost city\" + 0.000*\"elysium\" + 0.000*\"noise\" + 0.000*\"world arena\" + 0.000*\"bum\" + 0.000*\"fart\" + 0.000*\"uball\" + 0.000*\"gong fu\" + 0.000*\"gong\" + 0.000*\"arena issue\" + 0.000*\"fu\" + 0.000*\"gygax\" + 0.000*\"city gaxmoor\"'),\n",
       " (2,\n",
       "  '0.001*\"fact check\" + 0.000*\"extra spicy\" + 0.000*\"nz\" + 0.000*\"duck\" + 0.000*\"zorro\" + 0.000*\"brown\" + 0.000*\"guayusa\" + 0.000*\"check book\" + 0.000*\"mount starling\" + 0.000*\"starling\" + 0.000*\"tremolo\" + 0.000*\"nexus drawing\" + 0.000*\"drawing pad\" + 0.000*\"nexus\" + 0.000*\"mordux\" + 0.000*\"kittens\" + 0.000*\"add nz\" + 0.000*\"legendary duck\" + 0.000*\"catfight\" + 0.000*\"simulated environment\"'),\n",
       " (3,\n",
       "  '0.000*\"potr\" + 0.000*\"respio\" + 0.000*\"pots\" + 0.000*\"potr pots\" + 0.000*\"fpga\" + 0.000*\"mutant world\" + 0.000*\"rpi\" + 0.000*\"pmod\" + 0.000*\"helicopter\" + 0.000*\"governor\" + 0.000*\"steam governor\" + 0.000*\"audiocaption\" + 0.000*\"interstellar\" + 0.000*\"chagallpac\" + 0.000*\"beest\" + 0.000*\"salem\" + 0.000*\"artid\" + 0.000*\"son mutant\" + 0.000*\"gustavo\" + 0.000*\"page steam\"'),\n",
       " (4,\n",
       "  '0.001*\"moss\" + 0.000*\"lash\" + 0.000*\"pussycats\" + 0.000*\"jerky\" + 0.000*\"lash binder\" + 0.000*\"binder\" + 0.000*\"jerky gold\" + 0.000*\"eat death\" + 0.000*\"death naked\" + 0.000*\"simpson\" + 0.000*\"supplied unpainted\" + 0.000*\"lead free\" + 0.000*\"art simpson\" + 0.000*\"sandals\" + 0.000*\"tenkara\" + 0.000*\"lawn\" + 0.000*\"free pewter\" + 0.000*\"pewter\" + 0.000*\"unpainted\" + 0.000*\"supplied\"'),\n",
       " (5,\n",
       "  '0.000*\"sauce\" + 0.000*\"chat daddy\" + 0.000*\"chat\" + 0.000*\"honbinos\" + 0.000*\"daddy\" + 0.000*\"chateau\" + 0.000*\"mirobot\" + 0.000*\"pen\" + 0.000*\"ditko\" + 0.000*\"honbinos sauce\" + 0.000*\"creamgrip\" + 0.000*\"dojinshi\" + 0.000*\"ms\" + 0.000*\"turf\" + 0.000*\"life book\" + 0.000*\"turf grain\" + 0.000*\"clam\" + 0.000*\"hokey\" + 0.000*\"hokey pokey\" + 0.000*\"pokey\"'),\n",
       " (6,\n",
       "  '0.000*\"bummy\" + 0.000*\"elemental\" + 0.000*\"baby bummy\" + 0.000*\"og\" + 0.000*\"med\" + 0.000*\"til\" + 0.000*\"vi\" + 0.000*\"en\" + 0.000*\"denim\" + 0.000*\"som\" + 0.000*\"bass backing\" + 0.000*\"juice\" + 0.000*\"det\" + 0.000*\"feast\" + 0.000*\"er\" + 0.000*\"och\" + 0.000*\"thinkcar\" + 0.000*\"presets\" + 0.000*\"resin dice\" + 0.000*\"juice bar\"'),\n",
       " (7,\n",
       "  '0.009*\"pins\" + 0.007*\"pin\" + 0.004*\"enamel\" + 0.004*\"unlocked\" + 0.002*\"hard enamel\" + 0.001*\"hard\" + 0.001*\"enamel pin\" + 0.001*\"rubber\" + 0.001*\"plated\" + 0.001*\"unlocks\" + 0.001*\"clutches\" + 0.001*\"glitter\" + 0.001*\"gold\" + 0.001*\"rubber clutches\" + 0.001*\"backing\" + 0.001*\"locked\" + 0.001*\"backing card\" + 0.001*\"enamel pins\" + 0.001*\"gold plated\" + 0.001*\"unlocked unlocked\"'),\n",
       " (8,\n",
       "  '0.001*\"cookware\" + 0.001*\"pans\" + 0.001*\"device\" + 0.000*\"tinkle\" + 0.000*\"tinkle topper\" + 0.000*\"topper\" + 0.000*\"solidteknics\" + 0.000*\"toilet\" + 0.000*\"computing\" + 0.000*\"wrought\" + 0.000*\"computing device\" + 0.000*\"exercise device\" + 0.000*\"exercise\" + 0.000*\"jewel\" + 0.000*\"pee\" + 0.000*\"cast iron\" + 0.000*\"invention\" + 0.000*\"watchum\" + 0.000*\"wrought iron\" + 0.000*\"bipolar\"'),\n",
       " (9,\n",
       "  '0.003*\"album\" + 0.003*\"band\" + 0.002*\"songs\" + 0.002*\"record\" + 0.002*\"music\" + 0.001*\"daphne\" + 0.001*\"timbo\" + 0.001*\"kamishibai\" + 0.001*\"voodoo\" + 0.001*\"recording\" + 0.001*\"studio\" + 0.001*\"musicians\" + 0.001*\"trauma\" + 0.001*\"stan\" + 0.001*\"women\" + 0.001*\"blond\" + 0.001*\"blond voodoo\" + 0.001*\"book\" + 0.001*\"tour\" + 0.001*\"kclr\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kickstarter_corpus = df.story.values\n",
    "kwargs = dict(\n",
    "    ngram_range=(1, 2),  \n",
    "    stop_words='english',\n",
    "    token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    ")\n",
    "cv = CountVectorizer(**kwargs)\n",
    "cv.fit(kickstarter_corpus)\n",
    "counts = cv.transform(kickstarter_corpus).transpose()\n",
    "print(counts.shape)\n",
    "corpus = matutils.Sparse2Corpus(counts)\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=10, id2word=id2word, passes=10)\n",
    "lda.print_topics(num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "abortion reporting able come absolutely fine acadia black access intended ability magnetize access able download ability activate accelerates opposite\n",
      "\n",
      "Topic #1:\n",
      "absolute best able ship accessibility sustainable abilities ranked access studio able listen access clientele able share accelerator consensys access anime\n",
      "\n",
      "Topic #2:\n",
      "absurd comedy academy headmaster able attend abandoned disenfranchised abrupt change abs post able remove abomination accept people abilities aren\n",
      "\n",
      "Topic #3:\n",
      "access enhances able endure abbotsford smoking abiding accelerations spin abilities protect abilities willencrad accents really access discord absolute comics\n",
      "\n",
      "Topic #4:\n",
      "abbotsford local abstract theme ability match able reference access camera ability design abuse doesn able colors abandoned homes ability replicating\n"
     ]
    }
   ],
   "source": [
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Tweak the two parameters below\n",
    "number_topics = 5\n",
    "number_words = 10\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(counts)\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, cv, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Term frequencies and vocabulary are of different sizes, 2259 != 574379.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyLDAvis\\sklearn.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(lda_model, dtm, vectorizer, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyLDAvis\\sklearn.py\u001b[0m in \u001b[0;36m_extract_data\u001b[1;34m(lda_model, dtm, vectorizer)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mterm_freqs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         ('Term frequencies and vocabulary are of different sizes, {} != {}.'\n\u001b[1;32m---> 45\u001b[1;33m          .format(term_freqs.shape[0], len(vocab)))\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mtopic_term_dists\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Term frequencies and vocabulary are of different sizes, 2259 != 574379."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "LDAvis_data_filepath = os.path.join('ldavis_prepared_'+str(number_topics))\n",
    "\n",
    "LDAvis_prepared = sklearn_lda.prepare(lda, counts, cv)\n",
    "with open(LDAvis_data_filepath, 'w') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath) as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(number_topics) +'.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
